# Основи структур даних та пам'яті

## Зміст

1. [Вступ: за межами асимптотичного аналізу](#вступ-за-межами-асимптотичного-аналізу)
2. [Частина 1: Канонічні будівельні блоки - організація даних у пам'яті](#частина-1-канонічні-будівельні-блоки--організація-даних-у-памяті)
    - [Масиви: сила неперервності](#масиви-сила-неперервності)
    - [Зв'язні списки: гнучкість вказівників](#звязні-списки-гнучкість-вказівників)
3. [Частина 2: "Чому" - подорож усередину машини](#частина-2-чому--подорож-усередину-машини)
    - [Вказівники та керування пам'яттю](#вказівники-та-керування-памяттю)
    - [Ієрархія пам'яті та принцип локальності](#ієрархія-памяті-та-принцип-локальності)
    - [Велика прірва: продуктивність кешу](#велика-прірва-продуктивність-кешу)
4. [Частина 3: Емпіричне підтвердження - аналіз тестів продуктивності](#частина-3-емпіричне-підтвердження--аналіз-тестів-продуктивності)
    - [Тест 1: Послідовний обхід](#тест-1-послідовний-обхід)
    - [Тест 2: Довільний доступ](#тест-2-довільний-доступ)
    - [Тест 3: Вставка на початок](#тест-3-вставка-на-початок)
5. [Частина 4: Теорія на практиці - алгоритмічні патерни](#частина-4-теорія-на-практиці--алгоритмічні-патерни)
    - [Компроміс "час-простір": геш-таблиці](#компроміс-час-простір-геш-таблиці)
    - [Маніпуляції "на місці": техніка двох вказівників](#маніпуляції-на-місці-техніка-двох-вказівників)
    - [Маніпуляції з вказівниками: майстерність операцій зі зв'язними списками](#маніпуляції-з-вказівниками-майстерність-операцій-зі-звязними-списками)
    - [Хитрі алгоритми: виявлення циклів](#хитрі-алгоритми-виявлення-циклів)
    - [Просунуті патерни: купи для задач k-спрямованого злиття](#просунуті-патерни-купи-для-задач-k-спрямованого-злиття)
6. [Частина 5: Висновки та подальше вивчення](#частина-5-висновки-та-подальше-вивчення)
    - [Підсумки та ключові компроміси](#підсумки-та-ключові-компроміси)
    - [Рекомендовані ресурси та література](#рекомендовані-ресурси-та-література)
    - [Рекомендовані задачі для практики](#рекомендовані-задачі-для-практики)
7. [Підготовка до співбесіди та перевірка знань](#підготовка-до-співбесіди-та-перевірка-знань)
    - [У центрі уваги: пробна співбесіда](#у-центрі-уваги-пробна-співбесіда)
    - [Базові питання та відповіді](#базові-питання-та-відповіді)
    - [Тест для перевірки знань](#тест-для-перевірки-знань)
8. [Додаток: Вихідний код тестів продуктивності на C++](#додаток-вихідний-код-тестів-продуктивності-на-c)

---

## Вступ: за межами асимптотичного аналізу

Мета цієї фундаментальної лекції - вийти за рамки традиційного розуміння структур даних, яке часто починається і
закінчується асимптотичним аналізом, відомим як **нотація "великого О" (Big O)**. Хоча "велике О" є незамінним
інструментом для оцінки масштабованості алгоритмів, воно свідомо абстрагується від самої машини. Ця абстракція, хоч і
корисна, створює розрив у продуктивності між теоретичною складністю та реальною швидкістю виконання. Компетентний
інженер-програміст розуміє, що цикл `for`, який теоретично є ідентичним для двох різних структур даних, на практиці може
мати відмінності в продуктивності на порядки.

Ця розбіжність майже повністю пояснюється фізичними реаліями комп'ютерного обладнання, зокрема **ієрархією пам'яті**. Ця
лекція подолає розрив між кодом високого рівня та низькорівневими операціями машини - тактами процесора, кеш-влучаннями
та зверненнями до оперативної пам'яті, - які зрештою визначають продуктивність. Саме розуміння цієї взаємодії між
алгоритмом та архітектурою відрізняє компетентного програміста від інженера, що спеціалізується на високопродуктивних
системах.

Наприклад, обхід масиву та обхід зв'язного списку класифікуються як операції $`O(n)`$, що передбачає їхню порівнянну
продуктивність для заданої кількості елементів $`n`$. Однак емпіричні тести незмінно демонструють, що обхід масиву
значно швидший. Це виявляє критичне обмеження використання лише асимптотичного аналізу. Нотація "великого О" ігнорує
константні фактори, але коли "константний фактор", зумовлений взаємодією з апаратним забезпеченням, призводить до
100-кратної різниці у швидкості, він стає найважливішим фактором для критично важливого коду.

---

## Частина 1: Канонічні будівельні блоки - організація даних у пам'яті

### Масиви: сила неперервності

#### Фундаментальне визначення

**Масив** - це структура даних, що зберігає колекцію елементів одного типу в **неперервному блоці пам'яті**. Ця єдина
характеристика - неперервність - є фундаментальним джерелом як найбільших переваг масиву, так і його найзначніших
недоліків.

#### Статичні та динамічні масиви

Концепцію масиву можна розділити на дві основні категорії залежно від того, як керуються його розмір та пам'ять.

* **Статичні масиви**: Статичний масив має розмір, **встановлений на етапі компіляції**. Пам'ять для статичного масиву
  зазвичай виділяється на **стеку** - області пам'яті, відомій своїм надзвичайно швидким доступом, але обмеженим
  розміром. Це робить статичні масиви ідеальними для сценаріїв, де кількість елементів відома заздалегідь і не
  змінюватиметься.
* **Динамічні масиви**: Динамічний масив, навпаки, може змінювати свій розмір під час виконання програми. Його пам'ять
  виділяється з **купи** - набагато більшого та гнучкішого пулу пам'яті. У сучасних високорівневих мовах динамічні
  масиви часто є реалізацією за замовчуванням для спископодібних структур. Приклади включають `list` у
  Python, `std::vector` у C++ та `ArrayList` у Java.

Механізм, що дозволяє динамічному масиву зростати, є критично важливим для розуміння. Коли місткість масиву
перевищується, базова система виконує операцію зміни розміру:

1. На купі виділяється новий, більший неперервний блок пам'яті, часто вдвічі більший за попередній.
2. Усі елементи зі старого блоку пам'яті копіюються в новий.
3. Старий блок пам'яті звільняється.

Ця операція зміни розміру є обчислювально дорогою ($`O(n)`$), але оскільки вона відбувається нечасто, вартість
розподіляється на багато вставок. Через процес, що називається **амортизаційним аналізом**, середня вартість операції
додавання в кінець вважається константною, або **амортизованою $`O(1)`$**.

#### Фізика доступу за індексом за $`O(1)`$

Неперервність масивів забезпечує прямий доступ за константний час до будь-якого елемента через його індекс. Адресу
пам'яті будь-якого елемента можна обчислити за формулою:
$`адреса\_елемента = базова\_адреса + (індекс \times розмір\_одного\_елемента)`$

Це обчислення включає лише одне множення та одне додавання - операції, які процесор може виконати за одну інструкцію.
Час, необхідний для цього, абсолютно не залежить від розміру масиву, що робить його справжньою операцією **$`O(1)`$**.

#### Вартість зміни $`O(n)`$

Хоча доступ для читання є швидким, зміна структури масиву може бути повільною. Для вставки або видалення елемента на
початку або в середині необхідно зберігати властивість неперервності. Це вимагає зсуву всіх наступних елементів, що
призводить до лінійної часової складності **$`O(n)`$** для цих операцій.

### Зв'язні списки: гнучкість вказівників

#### Анатомія вузла

**Зв'язний список** - це лінійна структура даних, що складається з послідовності **вузлів**. На відміну від масивів, ці
вузли не зберігаються в пам'яті неперервно; вони можуть бути розкидані випадковим чином по всій купі. Кожен вузол
містить два основні компоненти:

* Самі **дані**.
* **Вказівник** - змінну, що зберігає адресу пам'яті наступного вузла.

#### Таксономія зв'язних списків

* **Однозв'язний список (SLL)**: Кожен вузол має один вказівник (`next`). Обхід можливий лише вперед. Має найменші
  накладні витрати пам'яті.
* **Двозв'язний список (DLL)**: Кожен вузол має два вказівники (`next` та `prev`). Обхід є двонаправленим. Це робить
  видалення відомого вузла операцією $`O(1)`$, оскільки його попередник миттєво доступний.
* **Кільцевий зв'язний список**: Вказівник `next` останнього вузла вказує назад на головний вузол, створюючи цикл.
  Корисний для циклічного планування (round-robin).

| Характеристика               | Однозв'язний список                 | Двозв'язний список            |
|:-----------------------------|:------------------------------------|:------------------------------|
| **Накладні витрати пам'яті** | Найнижчі (1 вказівник)              | Вищі (2 вказівники)           |
| **Обхід**                    | Однонаправлений (вперед)            | Двонаправлений                |
| **Видалення відомого вузла** | $`O(n)`$ (треба знайти попередника) | $`O(1)`$ (попередник відомий) |

#### Механіка вставки/видалення за $`O(1)`$

Основною перевагою зв'язного списку є його здатність виконувати вставки та видалення за **константний час ($`O(1)`$)**,
за умови, що місцезнаходження відоме. Для вставки нового елемента на початок потрібно лише кілька кроків:

1. Виділити пам'ять для нового вузла.
2. Встановити вказівник `next` нового вузла так, щоб він вказував на поточний головний вузол.
3. Оновити вказівник на голову списку, щоб він посилався на новий вузол.

Це включає невелику, фіксовану кількість перепризначень вказівників, що робить операцію $`O(1)`$.

---

## Частина 2: "Чому" - подорож усередину машини

### Вказівники та керування пам'яттю

**Вказівник** - це змінна, значенням якої є адреса в пам'яті. У мовах з ручним керуванням пам'яттю ця потужність пов'
язана з ризиками:

* **Висячі вказівники**: Вказівник, що посилається на місце в пам'яті, яке вже було звільнено. Доступ до нього
  призводить до невизначеної поведінки або збоїв.
* **Витоки пам'яті**: Виникають, коли програма виділяє пам'ять на купі, але втрачає всі вказівники на неї, що
  унеможливлює її звільнення.

Коротко кажучи: висячий вказівник - це валідний вказівник на невалідну пам'ять, тоді як витік пам'яті - це валідна
пам'ять без валідного вказівника. Сучасний C++ пом'якшує ці проблеми за допомогою розумних
вказівників (`std::unique_ptr`, `std::shared_ptr`).

### Ієрархія пам'яті та принцип локальності

Комп'ютерна ієрархія пам'яті організовує системи зберігання у вигляді піраміди рівнів, балансуючи між швидкістю,
вартістю та ємністю. Керівний принцип полягає в тому, щоб зберігати дані, до яких звертаються найчастіше, на найшвидших
і найдорожчих рівнях, а менш критичні дані - на повільніших, дешевших і більших рівнях.

#### Рівні пам'яті

Ієрархія зазвичай поділяється на три основні категорії: вбудована пам'ять, зовнішні накопичувачі та офлайн-сховища.

##### Вбудована пам'ять (Первинна пам'ять)

Це вся пам'ять, до якої процесор може отримати доступ безпосередньо, не використовуючи канали вводу-виводу (I/O). Це
найшвидша пам'ять у комп'ютері, але водночас найдорожча і з найменшою ємністю.

* **Компоненти:** Регістри процесора, кеш (L1, L2, L3) та основна пам'ять (RAM).
* **Особливості:**
    * **Енергозалежність:** Вбудована пам'ять зазвичай **енергозалежна**, тобто її вміст втрачається при вимкненні
      живлення.
    * **Швидкість:** Забезпечує надзвичайно швидкий час доступу, що вимірюється в наносекундах, дозволяючи процесору
      працювати без значних затримок.
    * **Доступність для CPU:** Це *єдине* сховище, з якого процесор може безпосередньо читати та писати. Дані з
      повільніших сховищ спочатку мають бути завантажені в RAM для обробки.
    * **Адресованість:** Є **байтово-адресованою**, тобто процесор може отримати доступ до будь-якого окремого байта
      даних.

##### Зовнішні накопичувачі (Вторинна пам'ять)

Це сховище, яке не є безпосередньо доступним для процесора і підключене до комп'ютера через контролери вводу-виводу.
Воно слугує основним сховищем для даних і програм, які наразі не використовуються активно.

* **Компоненти:** Твердотільні накопичувачі (SSD), жорсткі диски (HDD).
* **Особливості:**
    * **Енергонезалежність:** Зовнішні накопичувачі є **енергонезалежними**, тобто вони зберігають дані навіть після
      вимкнення живлення. Це робить їх ідеальними для довготривалого зберігання операційної системи, додатків та файлів
      користувача.
    * **Швидкість:** Значно повільніші за вбудовану пам'ять, з часом доступу, що вимірюється в мікросекундах (для SSD)
      або мілісекундах (для HDD).
    * **Ємність:** Пропонують набагато більшу ємність для зберігання за нижчою вартістю за байт порівняно з вбудованою
      пам'яттю.
    * **Адресованість:** Є **блоково-адресованими**, тобто дані читаються та записуються фіксованими блоками (наприклад,
      4 КБ), а не побайтово.

##### Офлайн-сховища (Третинна пам'ять)

Це тип сховища, що не перебуває під безпосереднім контролем процесора і часто вимагає втручання людини або робота для
його підключення та доступу. Використовується для архівування, резервного копіювання та транспортування великих обсягів
даних.

* **Компоненти:** Магнітні стрічки, оптичні диски (Blu-ray, DVD), зовнішні жорсткі диски або хмарні сервіси для
  архівування.
* **Особливості:**
    * **Знімність:** Визначальною особливістю є те, що носій інформації можна легко вийняти з приводу або системи.
    * **Швидкість:** Має найповільніший час доступу, який може становити від секунд до хвилин, оскільки може вимагати
      фізичного пошуку та завантаження стрічки або диска.
    * **Ємність:** Пропонує величезну ємність, що робить її придатною для резервного копіювання цілих систем.
    * **Вартість:** Має найнижчу вартість за байт серед усіх типів сховищ, що робить її економічно вигідною для
      зберігання величезних наборів даних.

---

##### Підсумок характеристик

| Характеристика          | Вбудована пам'ять               | Зовнішні накопичувачі        | Офлайн-сховища                   |
|:------------------------|:--------------------------------|:-----------------------------|:---------------------------------|
| **Швидкість доступу**   | Найшвидша (наносекунди)         | Середня (мікро/мілісекунди)  | Найповільніша (секунди-хвилини)  |
| **Енергозалежність**    | Енергозалежна                   | Енергонезалежна              | Енергонезалежна                  |
| **Ємність**             | Найменша (МБ-ГБ)                | Велика (ГБ-ТБ)               | Дуже велика (ТБ-Петабайти)       |
| **Вартість за байт**    | Найвища                         | Середня                      | Найнижча                         |
| **Доступність для CPU** | Пряма (байтово-адресована)      | Непряма (блоково-адресована) | Непряма (потребує підключення)   |
| **Типове використання** | Виконання програм, активні дані | Зберігання файлів, ОС        | Архівування, резервне копіювання |

---

#### Принцип локальності

Коли процесору потрібні дані, яких немає в його кеші (**кеш-промах**), він завантажує з оперативної пам'яті блок
неперервної пам'яті фіксованого розміру - **кеш-лінію** (зазвичай 64 байти). Ця система ефективна завдяки **принципу
локальності**: тенденції програм багаторазово звертатися до одних і тих самих або сусідніх ділянок пам'яті.

* **Часова локальність**: Якщо програма звертається до ділянки пам'яті, вона, ймовірно, незабаром звернеться до неї
  знову.
* **Просторова локальність**: Якщо програма звертається до ділянки пам'яті, вона, ймовірно, незабаром звернеться до
  сусідніх ділянок.

### Велика прірва: продуктивність кешу

Це єдина найважливіша причина, чому масиви в реальному світі часто перевершують зв'язні списки.

#### Масиви та просторова локальність (Каскад кеш-влучань)

При ітерації по масиву патерн доступу ідеально узгоджується з архітектурою процесора.

* Перший доступ (`array[0]`) викликає кеш-промах.
* Апаратне забезпечення завантажує всю кеш-лінію, що містить `array[0]`, яка також завантажує `array[1]`, `array[2]` і
  т.д. у надшвидкий кеш.
* Наступні доступи (`array[1]`, `array[2]`, ...) тепер є **кеш-влучаннями**, які на порядки швидші.
* Апаратний **механізм попередньої вибірки** процесора виявляє цей послідовний патерн і проактивно завантажує наступні
  кеш-лінії.
  Результатом є майже безперервний потік кеш-влучань, що дозволяє процесору обробляти дані на максимальній швидкості.

#### Зв'язні списки та кеш-промахи (Каскад переходів за вказівниками)

Обхід зв'язного списку є ворожим до архітектури кешування.

* Доступ до першого вузла викликає кеш-промах, і його кеш-лінія завантажується.
* Щоб перейти до наступного елемента, програма слідує за вказівником на нову адресу пам'яті. Оскільки вузли розкидані,
  цей наступний вузол майже напевно не знаходиться в поточній кеш-лінії.
* Це призводить до ще одного **кеш-промаху**, змушуючи робити ще один повільний запит до оперативної пам'яті.
  Цей патерн повторюється майже для кожного вузла. Таким чином, обхід зв'язного списку може перетворитися на серію
  з $`n`$ кеш-промахів, причому процесор зупиняється на кожному кроці.

---

## Частина 3: Емпіричне підтвердження - аналіз тестів продуктивності

Теорія є важливою, але емпіричні дані надають незаперечні докази. Наступні тести продуктивності були виконані на C++ з
використанням `std::vector` (динамічний масив) та `std::list` (двозв'язний список), щоб продемонструвати реальний вплив
розміщення даних у пам'яті.

### Тест 1: Послідовний обхід

Цей тест ітерує по кожному елементу контейнера з 10 мільйонами цілих чисел і сумує їхні значення.
**Гіпотеза**: `std::vector` буде значно швидшим завдяки своєму неперервному розміщенню в пам'яті, що забезпечує відмінну
продуктивність кешу. `std::list` буде набагато повільнішим через переходи за вказівниками та часті кеш-промахи.

> **Типовий результат:**
> * Час `std::vector`: ~12.51 мс
> * Час `std::list`: ~110.84 мс

**Аналіз**: Вектор майже **в 9 разів швидший**. Ця приголомшлива різниця для теоретично ідентичної операції $`O(n)`$ є
прямим наслідком просторової локальності. Послідовний патерн доступу до пам'яті вектора тримає кеш процесора заповненим
релевантними даними, що призводить до високої частоти кеш-влучань. Навпаки, розкидані вузли списку викликають кеш-промах
майже для кожного елемента, змушуючи процесор постійно чекати дані з повільної основної пам'яті.

### Тест 2: Довільний доступ

Цей тест включає доступ до 100 000 елементів у випадкових позиціях у контейнері.
**Гіпотеза**: `std::vector` буде на порядки швидшим завдяки доступу за індексом за $`O(1)`$. `std::list` буде
надзвичайно повільним, оскільки кожен доступ вимагає обходу від початку за $`O(n)`$.

> **Типовий результат:**
> * Час `std::vector`: ~2.15 мс
> * Час `std::list`: ~7854.21 мс

**Аналіз**: Вектор **більш ніж у 3600 разів швидший**. Цей результат ідеально ілюструє алгоритмічну різницю. Вектор
виконує просте обчислення адреси для кожного доступу. Список, не маючи прямого доступу, повинен починати з голови і
проходити тисячі вузлів для кожного зі 100 000 запитів, що призводить до катастрофічно низької продуктивності.

### Тест 3: Вставка на початок

Цей тест вимірює час, необхідний для вставки 100 000 нових елементів на початок контейнера.
**Гіпотеза**: `std::list` буде значно швидшим завдяки своїй здатності до вставки за $`O(1)`$. `std::vector` буде
повільним, оскільки кожна вставка є операцією $`O(n)`$, що вимагає зсуву всіх існуючих елементів.

> **Типовий результат:**
> * Час `std::vector`: ~789.93 мс
> * Час `std::list`: ~1.95 мс

**Аналіз**: Список **більш ніж у 400 разів швидший**. Цей тест підкреслює головну перевагу зв'язного списку. Кожна
вставка - це просте оновлення вказівника, операція з константним часом. Вектор, навпаки, повинен зсувати весь свій вміст
для кожної вставки, що призводить до квадратичної загальної складності для тесту та жахливої продуктивності.

---

## Частина 4: Теорія на практиці - алгоритмічні патерни

### Компроміс "час-простір": геш-таблиці

Задачі, такі як [*Two Sum*](https://leetcode.com/problems/two-sum/) (Easy) та [*Contains
Duplicate*](https://leetcode.com/problems/contains-duplicate/) (Easy), підкреслюють класичний компроміс.

* **Груба сила ($`O(n^2)`$ часу, $`O(1)`$ простору)**: Використовуйте вкладені цикли. Просто, але повільно.
* **Оптимізований ($`O(n)`$ часу, $`O(n)`$ простору)**: Використовуйте геш-таблицю. Пройдіться по масиву один раз,
  зберігаючи побачені елементи. Для кожного елемента `x` перевірте, чи існує в таблиці `target - x` (для Two Sum) або
  сам `x` (для Contains Duplicate). Це обмінює додаткову пам'ять на величезний виграш у швидкості. Цей патерн також
  поширюється на такі задачі, як [*Contains Duplicate II*](https://leetcode.com/problems/contains-duplicate-ii/) (Easy).

### Маніпуляції "на місці": техніка двох вказівників

Для задач на кшталт [*Remove Duplicates from Sorted
Array*](https://leetcode.com/problems/remove-duplicates-from-sorted-array/) (Easy) та її складнішого продовження [
*Remove Duplicates from Sorted Array II*](https://leetcode.com/problems/remove-duplicates-from-sorted-array-ii/) (
Medium) з обмеженням простору $`O(1)`$ ідеально підходить **техніка двох вказівників**.

* **Повільний вказівник** (`write_index`) відстежує позицію для наступного унікального елемента.
* **Швидкий вказівник** (`read_index`) проходить через весь масив.
  Якщо `nums[read_index]` унікальний, він копіюється в `nums[write_index]`, і обидва вказівники просуваються. Якщо це
  дублікат, просувається лише `read_index`.

### Маніпуляції з вказівниками: майстерність операцій зі зв'язними списками

* **[*Reverse Linked List*](https://leetcode.com/problems/reverse-linked-list/) (Easy)**: Класичне ітеративне рішення
  вимагає трьох вказівників: `previous`, `current` та `next_temp`. У циклі ви розвертаєте вказівник `next` поточного
  вузла, щоб він вказував на `previous`, не втрачаючи решту списку. Складніший варіант - [*Reverse Linked List
  II*](https://leetcode.com/problems/reverse-linked-list-ii/) (Medium), який розвертає певний сегмент списку.
* **[*Merge Two Sorted Lists*](https://leetcode.com/problems/merge-two-sorted-lists/) (Easy)**: Використовуйте фіктивний
  головний вузол, щоб спростити крайні випадки. Вказівник `current` будує новий список, порівнюючи вузли двох вхідних
  списків і додаючи менший з них. Ця концепція масштабується до поширеної складної задачі *
  *[*Merge k Sorted Lists*](https://leetcode.com/problems/merge-k-sorted-lists/)**, яка оптимально вирішується за
  допомогою мін-купи.

### Хитрі алгоритми: виявлення циклів

Задача [*Linked List Cycle*](https://leetcode.com/problems/linked-list-cycle/) (Easy) має оптимальне рішення, яке
вимагає більшого, ніж простого обходу.

* **Геш-сет ($`O(n)`$ часу, $`O(n)`$ простору)**: Зберігайте відвідані вузли в сеті. Якщо вузол зустрічається двічі,
  існує цикл.
* **Алгоритм Флойда для пошуку циклів ($`O(n)`$ часу, $`O(1)`$ простору)**: "Черепаха та заєць". Використовуйте
  повільний вказівник (рухається на 1 крок) і швидкий вказівник (рухається на 2 кроки). Якщо є цикл, швидкий вказівник
  гарантовано зрештою наздожене повільного. Поширене продовження, [*Linked List Cycle
  II*](https://leetcode.com/problems/linked-list-cycle-ii/) (Medium), вимагає знайти точний вузол, де починається цикл.

### Просунуті патерни: купи для задач k-спрямованого злиття

Задачі, що включають кілька відсортованих списків, часто виграють від використання **мін-купи** (або пріоритетної черги)
для ефективного відстеження найменшого елемента серед усіх списків. Це ключ до вирішення таких задач, як [*Merge k
Sorted Lists*](https://leetcode.com/problems/merge-k-sorted-lists/) (Hard) та ще більш просунутої [*Smallest Range
Covering Elements from k Lists*](https://leetcode.com/problems/smallest-range-covering-elements-from-k-lists/) (Hard).

---

## Частина 5: Висновки та подальше вивчення

### Підсумки та ключові компроміси

| Характеристика                   | Масив (динамічний, напр., `std::vector`)                  | Зв'язний список (двозв'язний)                                         |
|:---------------------------------|:----------------------------------------------------------|:----------------------------------------------------------------------|
| **Розташування в пам'яті**       | ✅ Неперервне                                              | ❌ Розкидане                                                           |
| **Доступ за індексом**           | ✅ $`O(1)`$                                                | ❌ $`O(n)`$                                                            |
| **Вставка/Видалення (середина)** | ❌ $`O(n)`$                                                | ✅ $`O(1)`$ (якщо вузол відомий)                                       |
| **Вставка/Видалення (кінець)**   | ✅ Амортизований $`O(1)`$                                  | ✅ $`O(1)`$                                                            |
| **Продуктивність кешу**          | ✅ Відмінна                                                | ❌ Погана                                                              |
| **Накладні витрати пам'яті**     | Низькі                                                    | Високі (вказівники)                                                   |
| **Коли використовувати**         | Вибір за замовчуванням. Довільний доступ. Часті ітерації. | Часті вставки/видалення на кінцях. Потрібна стабільність вказівників. |

### Рекомендовані ресурси та література

#### Основні концепції та підручники

* ***Computer Systems: A Programmer’s Perspective (Bryant & O’Hallaron)***
    * **Розділи 6.2 та 6.5**: Зосередьтеся на локальності та кеш-пам'яті, щоб зрозуміти, *чому* послідовні структури
      даних, такі як масиви, так добре працюють з кешем.
    * **Розділ 3**: Пояснює різницю між стеком і купою та як структури даних розміщуються в пам'яті.
* ***Memory Systems: Cache, DRAM, Disk (Jacob et al.)***
    * **Розділ 1**: Надає огляд ієрархії пам'яті та її впливу на продуктивність, підкреслюючи важливість кеш-локальності
      в дебатах "масив проти списку".
* ***Modern Operating Systems (Tanenbaum)***
    * **Розділ 1.5.1 (Processes > Address Space)**: Корисно для розуміння того, як організована пам'ять для процесів.
* ***Introduction to Algorithms (CLRS)*** та ***The Algorithm Design Manual (Skiena)***: Важливі тексти для глибокого
  занурення в алгоритми та структури даних.

#### Онлайн-ресурси та візуалізації

* **Стаття на GeeksforGeeks**: *"Why Arrays have better cache locality than Linked Lists"* пропонує просте, засноване на
  прикладах пояснення того, як послідовне розміщення в пам'яті прискорює програми.
* **Візуалізації**: USFCA Visualizations, DataStructures.live та Y. Daniel Liang's Animations чудово підходять для
  візуалізації роботи цих структур.

### Рекомендовані задачі для практики

Ось список рекомендованих задач з LeetCode для закріплення вашого розуміння, відсортованих за складністю.

#### Легкі

- [*Two Sum*](https://leetcode.com/problems/two-sum/description/)
- [*Contains Duplicate*](https://leetcode.com/problems/contains-duplicate/description/)
- [*Contains Duplicate II*](https://leetcode.com/problems/contains-duplicate-ii/description/)
- [*Reverse Linked List*](https://leetcode.com/problems/reverse-linked-list/description/)
- [*Linked List Cycle*](https://leetcode.com/problems/linked-list-cycle/description/)
- [*Remove Duplicates from Sorted
  Array*](https://leetcode.com/problems/remove-duplicates-from-sorted-array/description/)
- [*Merge Two Sorted Lists*](https://leetcode.com/problems/merge-two-sorted-lists/)

#### Середні

- [*Linked List Cycle II*](https://leetcode.com/problems/linked-list-cycle-ii/description/)
- [*Remove Duplicates from Sorted Array
  II*](https://leetcode.com/problems/remove-duplicates-from-sorted-array-ii/description/)
- [*Reverse Linked List II*](https://leetcode.com/problems/reverse-linked-list-ii/description/)

#### Складні

- [*Merge k Sorted Lists*](https://leetcode.com/problems/merge-k-sorted-lists/description/)
- [*Smallest Range Covering Elements from k
  Lists*](https://leetcode.com/problems/smallest-range-covering-elements-from-k-lists/)

---

## Підготовка до співбесіди та перевірка знань

### У центрі уваги: пробна співбесіда

> **Питання**: "На сучасному обладнанні, чому масиви часто швидші за зв'язні списки для обходу, хоча обидва мають
> складність $`O(n)`$?"
>
> **Фокус відповіді**: **Кеш-локальність**. Поясніть, що масиви мають відмінну просторову локальність. Коли
> завантажується `array[0]`, кеш процесора також отримує `array[1]`, `array[2]` і т.д., що призводить до кеш-влучань.
> Вузли зв'язного списку розкидані, що викликає кеш-промах майже для кожного вузла. Використовуйте аналогію "кухонна
> стільниця проти комори".

> **Питання**: "Коли зв'язний список може бути кращим вибором, ніж масив (або `std::vector`)?"
>
> **Фокус відповіді**: Вийдіть за рамки "швидких вставок". Ключові випадки використання: 1) **Стабільність
вказівників/ітераторів**: зміна розміру `std::vector` робить вказівники недійсними. 2) **Справжня черга/дек за $`O(1)`$
**: реалізація черг, де вставки/видалення відбуваються строго на кінцях. 3) **Дуже великі об'єкти**: вартість зсуву
> великих об'єктів у масиві може бути надмірною.

> **Питання**: "Поясніть, що таке вказівник і яку роль він відіграє в доступі до пам'яті."
>
> **Фокус відповіді**: Вказівник - це змінна, що зберігає адресу пам'яті. Замість того, щоб містити дані безпосередньо,
> він містить місцезнаходження *цих* даних. Щоб отримати доступ до даних, система повинна "розіменувати" вказівник, тобто
> подивитися на адресу, що зберігається у вказівнику, щоб знайти фактичне значення в RAM. Вони є фундаментальним
> механізмом для зв'язних списків та ручного керування пам'яттю на купі.

> **Питання**: "Які проблеми можуть виникнути через неправильне керування вказівниками?"
>
> **Фокус відповіді**: Дві основні проблеми - це **висячі вказівники** та **витоки пам'яті**. Використовуйте аналогію "
> протилежностей". Висячий вказівник - це валідний вказівник на невалідну (звільнену) пам'ять, що часто призводить до
> збою. Витік пам'яті - це валідна (виділена) пам'ять, на яку немає валідного вказівника, що часто призводить до
> поступового уповільнення та вичерпання ресурсів.

### Базові питання та відповіді

**1. Що таке масив, і чому його називають "неперервним"?**
Масив - це структура даних, подібна до книжкової полиці, де всі її елементи зберігаються пліч-о-пліч в одному цілісному
блоці пам'яті. Його називають "неперервним", тому що між його елементами немає прогалин; якщо перший елемент знаходиться
за адресою пам'яті 1000, наступний (для 4-байтного цілого числа) буде за адресою 1004, потім 1008 і так далі.

**2. Яка основна різниця між статичним і динамічним масивом?**
Основна різниця полягає в тому, коли і як визначається їхній розмір.

* **Статичний масив** має фіксований розмір, який встановлюється під час написання програми (на етапі компіляції) і не
  може бути змінений. Він зазвичай зберігається у швидкій, але обмеженій області пам'яті під назвою стек.
* **Динамічний масив** (наприклад, `std::vector` у C++ або `list` у Python) може змінювати свій розмір під час виконання
  програми. Він зберігається у більшій, гнучкішій області пам'яті під назвою купа.

**3. Як "зростає" динамічний масив (наприклад, `std::vector` або `list` у Python)? Чи це повільно?**
Коли в динамічному масиві закінчується місце, він виконує операцію зміни розміру: створює новий, більший масив (зазвичай
вдвічі більший), копіює всі старі елементи в новий, а потім видаляє старий. Ця операція копіювання сама по собі
повільна (складність $`O(n)`$). Однак, оскільки це відбувається не дуже часто, середня вартість додавання елемента
вважається дуже швидкою, або амортизованою $`O(1)`$.

**4. Чому доступ до елемента масиву за його індексом (напр., `my_array[i]`) такий швидкий?**
Це швидко, тому що комп'ютер не шукає елемент; він миттєво обчислює його точну адресу в пам'яті. Він використовує просту
математичну формулу: `адреса_елемента = початкова_адреса_масиву + (індекс × розмір_одного_елемента)`. Це обчислення
займає постійний час, незалежно від розміру масиву, тому його складність становить $`O(1)`$.

**5. Що таке зв'язний список, і чим він відрізняється від масиву в пам'яті?**
Зв'язний список схожий на полювання за скарбами. Його елементи (звані вузлами) розкидані по всій пам'яті й не
зберігаються пліч-о-пліч. Кожен вузол містить дані та "вказівник" (адресу), який вказує, де знайти наступний вузол у
ланцюжку.

**6. Що таке "вузол" у зв'язному списку?**
Вузол - це основний будівельний блок зв'язного списку. Він складається з двох частин:

* **Дані**: Інформація, яку ми хочемо зберегти (наприклад, число, ім'я).
* **Вказівник(и)**: Адреса(и) пам'яті, що вказують на наступний (а іноді й на попередній) вузол у списку.

**7. Яка різниця між однозв'язним і двозв'язним списком?**

* **Однозв'язний список** має лише один вказівник у кожному вузлі, який вказує на наступний елемент. Ви можете рухатися
  по ньому лише вперед.
* **Двозв'язний список** має два вказівники: один на наступний елемент, а інший - на попередній. Це дозволяє рухатися як
  вперед, так і назад по списку, що робить деякі операції, наприклад видалення поточного вузла, ефективнішими.

**8. Чому додавання нового елемента на початок зв'язного списку таке швидке?**
Тому що не потрібно нічого зсувати. Щоб додати елемент на початок, ви просто виконуєте кілька швидких операцій:
створюєте новий вузол, змушуєте його вказувати на стару "голову" списку, а потім оголошуєте цей новий вузол новою "
головою". Це займає постійний час ($`O(1)`$), незалежно від довжини списку.

**9. Що краще: масив чи зв'язний список?**
Це залежить від завдання.

* **Масив** кращий, коли вам потрібен швидкий доступ до елементів за індексом і ви рідко вставляєте або видаляєте
  елементи з середини. Завдяки кращій продуктивності кешу, він майже завжди швидший для ітерації по елементах.
* **Зв'язний список** кращий, коли вам потрібно часто і швидко вставляти або видаляти елементи з початку або кінця
  списку (наприклад, для реалізації черги).

**10. Що таке вказівник? Наведіть аналогію з реального світу.**
Вказівник - це змінна, яка зберігає не самі дані, а адресу в пам'яті, де ці дані знаходяться.
**Аналогія**: Номер сторінки в змісті книги. Номер сторінки - це не сама інформація, але він вказує, де її знайти. Щоб
отримати інформацію, ви "розіменовуєте" вказівник - тобто, переходите на сторінку, на яку він вказує.

**11. Яка різниця між 32-бітним і 64-бітним комп'ютером?**
Основна різниця полягає в тому, скільки пам'яті (RAM) може адресувати комп'ютер.

* **32-бітний** комп'ютер використовує 32 біти для адрес пам'яті, що дозволяє йому працювати з максимум $`2^{32}`$
  байтів, або 4 ГБ оперативної пам'яті.
* **64-бітний** комп'ютер використовує 64 біти, що дозволяє йому адресувати $`2^{64}`$ байтів - величезну кількість
  пам'яті (16 ексабайтів), набагато більше, ніж потрібно сучасним комп'ютерам. Це також дозволяє процесору працювати з
  більшими порціями даних за раз.

**12. Який розмір вказівника (в байтах)?**
Розмір вказівника залежить від архітектури комп'ютера. На 32-бітній системі вказівник займає 32 біти (4 байти). На
64-бітній системі він займає 64 біти (8 байтів). Це логічно, оскільки вказівник повинен бути достатньо великим, щоб
зберігати будь-яку можливу адресу пам'яті в системі.

**13. Яка різниця між стеком і купою?**
Стек і купа - це дві основні області пам'яті, які використовує програма.

* **Стек**: Дуже швидка, невелика область пам'яті для статичних даних, локальних змінних і викликів функцій. Пам'ять тут
  керується автоматично (змінні створюються при вході у функцію і видаляються при виході з неї).
* **Купа**: Велика область пам'яті для динамічних даних, розмір яких не відомий на етапі компіляції (наприклад, для
  динамічних масивів або вузлів зв'язного списку). Пам'ять на купі повинна виділятися і звільнятися вручну (або збирачем
  сміття в деяких мовах).

**14. Що таке "витік пам'яті" і чому це погано?**
Витік пам'яті - це ситуація, коли програма виділяє пам'ять на купі, але потім втрачає всі вказівники на неї, не
звільняючи її. Ця пам'ять стає "сиротою" - вона зайнята, але недоступна. Це погано, тому що з часом ці витоки
накопичуються, зменшуючи доступну пам'ять, що може сповільнити або навіть призвести до збою програми чи всієї системи.

**15. Що таке "висячий вказівник" і чим він відрізняється від витоку пам'яті?**
Висячий вказівник - це вказівник, який продовжує вказувати на місце в пам'яті, яке вже було звільнено. Спроба
використати такий вказівник призводить до непередбачуваної поведінки або збою. Вони є протилежностями:

* **Витік пам'яті**: Валідна пам'ять, на яку немає вказівника.
* **Висячий вказівник**: Валідний вказівник, що вказує на невалідну пам'ять.

**16. Що таке "ієрархія пам'яті" і навіщо вона нам потрібна?**
Це багаторівнева система пам'яті в комп'ютері, організована як піраміда. На вершині знаходиться найшвидша, але найменша
пам'ять (регістри процесора, кеш), а внизу - найповільніша, але найбільша (RAM, диски). Вона потрібна для балансування
швидкості, вартості та розміру, оскільки дуже швидка пам'ять є дорогою. Ієрархія дозволяє зберігати дані, що
використовуються найчастіше, на найшвидших рівнях.

**17. Що таке кеш процесора? Чому він такий важливий для швидкості?**
Кеш - це невеликий, надзвичайно швидкий буфер пам'яті, розташований дуже близько до процесора. Він важливий, тому що
доступ до основної пам'яті (RAM) дуже повільний для сучасних процесорів. Кеш зберігає копії даних, що використовуються
найчастіше, дозволяючи процесору отримувати їх майже миттєво замість того, щоб чекати на повільну RAM.

**18. Що таке кеші L1, L2 та L3?**
Це різні рівні кеш-пам'яті з різною швидкістю та розміром.

* **Кеш L1**: Найменший і найшвидший, вбудований безпосередньо в кожне ядро процесора.
* **Кеш L2**: Більший, але трохи повільніший за L1. Може бути індивідуальним для кожного ядра або спільним.
* **Кеш L3**: Найбільший і найповільніший з трьох, зазвичай спільний для всіх ядер процесора.

**19. Що таке "кеш-лінія"?**
Це найменший блок даних (зазвичай 64 байти), який може передаватися між основною пам'яттю та кешем. Коли процесор
запитує один байт даних, він насправді завантажує всю 64-байтну кеш-лінію, що містить цей байт.

**20. Що таке "кеш-влучання" і "кеш-промах"?**

* **Кеш-влучання**: Процесор знаходить потрібні йому дані в кеші. Це дуже швидка операція.
* **Кеш-промах**: Процесор не знаходить дані в кеші і змушений звертатися до набагато повільнішої основної пам'яті (
  RAM). Це значно сповільнює операцію.

**21. Що таке "просторова локальність" і "часова локальність"?**
Це два типи передбачуваної поведінки програм, які дозволяють кешу бути ефективним.

* **Часова локальність**: Якщо програма звертається до даних, вона, ймовірно, незабаром звернеться до них знову (
  наприклад, змінна в циклі).
* **Просторова локальність**: Якщо програма звертається до даних, вона, ймовірно, незабаром звернеться до даних,
  розташованих поруч у пам'яті (наприклад, до наступного елемента в масиві).

**22. Чому ітерація по масиву майже завжди швидша, ніж по зв'язному списку?**
Через просторову локальність і кеш. Елементи масиву розташовані послідовно в пам'яті. Коли процесор завантажує один
елемент, він автоматично завантажує його сусідів у кеш (в тій самій кеш-лінії). Тому наступні доступи стають
кеш-влучаннями. Вузли зв'язного списку розкидані, тому кожен доступ до наступного вузла, ймовірно, буде кеш-промахом.

**23. Що таке "тактова частота" і "цикли процесора"? Як вони пов'язані зі швидкістю?**

* **Тактова частота** (вимірюється в гігагерцах, ГГц) - це швидкість, з якою процесор виконує базові операції. 3 ГГц
  означає 3 мільярди тактів (циклів) на секунду.
* **Цикл процесора** - це найменша одиниця часу для процесора. Прості операції (як додавання) можуть займати один цикл,
  тоді як складні (як доступ до RAM) - сотні. Чим вища частота і чим менше циклів займає операція, тим швидше працює
  програма.

**24. Як різні типи даних, такі як цілі числа і символи, представлені в пам'яті?**
Різні типи даних займають різну кількість місця. Зазвичай:

* Ціле число (`int`) займає 4 байти.
* Символ (`char`) займає 1 байт.
* Число з плаваючою комою (`float`) займає 4 байти, а `double` - 8 байтів. Знання цього допомагає зрозуміти, як
  структури даних, особливо масиви, розміщуються в пам'яті.

**25. Моя програма використовує адреси, але чи є вони реальними фізичними адресами на чіпах RAM? (Віртуальна проти
фізичної пам'яті)**
Ні, зазвичай це віртуальні адреси. Операційна система створює для кожної програми ілюзію, що вона має весь адресний
простір для себе. Спеціальний апаратний компонент у процесорі (MMU - блок керування пам'яттю) перетворює ці віртуальні
адреси в реальні фізичні адреси в RAM. Це дозволяє ОС ефективно керувати пам'яттю, захищати програми одна від одної та
використовувати більше пам'яті, ніж є фізично (використовуючи файл підкачки). Цей процес перетворення додає крихітну
затримку, але переваги значно її перевершують.

**26. Що таке геш-таблиця (або геш-мапа)? Як вона працює?**
Геш-таблиця - це структура даних, яка зберігає пари ключ-значення і забезпечує дуже швидкий пошук, вставку та
видалення (в середньому $`O(1)`$). Вона використовує геш-функцію для перетворення ключа (наприклад, імені) в індекс у
внутрішньому масиві. Значення потім зберігається за цим індексом.

**27. Що таке "колізія геш-функції" і як її можна вирішити?**
Колізія виникає, коли геш-функція генерує однаковий індекс для двох різних ключів. Одним з найпоширеніших рішень є *
*метод ланцюжків**: замість зберігання одного елемента в кожному індексі масиву, зберігається зв'язний список. Якщо
виникає колізія, новий елемент просто додається до цього зв'язного списку.

**28. Для чого використовується "техніка двох вказівників"?**
Це поширений алгоритмічний патерн, особливо для масивів, який використовує два вказівники (індекси) для обходу або зміни
даних за один прохід. Наприклад, у задачі "Видалити дублікати з відсортованого масиву" один вказівник (`write_index`)
відстежує, куди записати наступний унікальний елемент, тоді як інший (`read_index`) сканує весь масив.

**29. Як алгоритм "Черепаха та заєць" виявляє цикл у зв'язному списку, не використовуючи додаткову пам'ять?**
Він використовує два вказівники: "повільний" (черепаха), який рухається на один крок за раз, і "швидкий" (заєць), який
рухається на два кроки за раз.

* Якщо циклу немає, швидкий вказівник просто досягне кінця списку.
* Якщо цикл є, обидва вказівники врешті-решт увійдуть до нього. Оскільки швидкий вказівник наздоганяє повільного, він
  неминуче "обжене" його на коло, і вони зустрінуться в одному вузлі. Ця зустріч є доказом існування циклу.

**30. Що означає, коли операція має складність $`O(1)`$, $`O(n)`$ або $`O(n^2)`$?**
Це спосіб описати, як час виконання алгоритму зростає зі збільшенням обсягу вхідних даних (n).

* **$`O(1)`$ (Константний час)**: Час виконання не залежить від розміру даних (напр., доступ до елемента масиву за
  індексом).
* **$`O(n)`$ (Лінійний час)**: Час виконання зростає прямо пропорційно до обсягу даних (напр., ітерація по всіх
  елементах списку).
* **$`O(n^2)`$ (Квадратичний час)**: Час виконання зростає пропорційно квадрату обсягу даних (напр., вкладені цикли, що
  порівнюють кожен елемент з кожним іншим).

### Тест для перевірки знань

**Питання 1:**
Додаток обробляє величезний набір даних з датчиків, і найпоширенішою операцією є послідовна ітерація по всіх показаннях
від початку до кінця. Чому масив буде значно швидшим за зв'язний список для цього конкретного завдання в реальному
сценарії?

* A. Неперервне розташування масиву в пам'яті дозволяє процесору завантажувати кілька сусідніх елементів у кеш
  одночасно, що призводить до великої кількості кеш-влучань.
* B. Масив використовує менше пам'яті в цілому, оскільки йому не потрібно зберігати вказівники для кожного елемента.
* C. Складність $`O(n)`$ для обходу масиву має менший константний фактор, ніж складність $`O(n)`$ для обходу зв'язного
  списку.
* D. Доступ до елемента в масиві за допомогою індексу є операцією $`O(1)`$, що робить цикл швидшим.

**Питання 2:**
Ви реалізуєте функцію "скасувати" (undo) для текстового редактора. Вам потрібно часто додавати та видаляти операції з
початку послідовності. Яка структура даних теоретично та практично найкраще підходить для цього, і чому?

* A. Зв'язний список, оскільки вставка або видалення на початку є операцією $`O(1)`$, яка вимагає лише оновлення кількох
  вказівників.
* B. Масив, оскільки він забезпечує доступ до будь-якого елемента за $`O(1)`$, що корисно для керування операціями.
* C. Зв'язний список, оскільки його елементи розкидані в пам'яті, що є ефективнішим для сучасних процесорів.
* D. Масив, оскільки додавання елементів вимагає лише одного виділення пам'яті, що дуже швидко.

**Питання 3:**
Що є основною причиною "кеш-промаху" при обході стандартного зв'язного списку?

* A. Наступний вузол у послідовності, ймовірно, знаходиться в абсолютно іншому, неперервному місці пам'яті, якого немає
  в поточній кеш-лінії.
* B. Процесор не може обчислити адресу пам'яті наступного вузла і повинен чекати на операційну систему.
* C. Дані всередині кожного вузла занадто великі, щоб поміститися в кеш процесора.
* D. Вказівники займають занадто багато місця в кеші, не залишаючи місця для фактичних даних.

**Питання 4:**
Програма зберігає колекцію об'єктів у пам'яті. З часом код втрачає всі вказівники та посилання на конкретний об'єкт. Як
називається ця ситуація?

* A. Витік пам'яті
* B. Висячий вказівник
* C. Помилка сегментації
* D. Кеш-промах

**Питання 5:**
В алгоритмі Флойда "Черепаха та заєць" для пошуку циклів, як виявляється цикл у зв'язному списку?

* A. Швидкий вказівник зрештою обганяє повільного на коло, і вони зустрічаються в одному вузлі.
* B. Повільний вказівник досягає кінця списку (null) раніше за швидкого.
* C. Швидкий вказівник змінює напрямок і зустрічає повільного.
* D. Відстань між швидким і повільним вказівниками стає постійною величиною.

#### Ключ до відповідей та пояснення

1. **Правильна відповідь: A.** Обґрунтування: Це безпосередньо пояснює приріст продуктивності, пов'язуючи неперервне
   розташування з механізмом кешу процесора та принципом просторової локальності. Хоча B і C є правдивими твердженнями,
   вони не є *основною* причиною драматичної різниці у швидкості.
2. **Правильна відповідь: A.** Обґрунтування: Це правильно визначає ідеальну структуру даних і пов'язує її з
   фундаментальною можливістю вставки/видалення за $`O(1)`$ на початку послідовності.
3. **Правильна відповідь: A.** Обґрунтування: Це правильно визначає, що переходи за вказівниками між розкиданими вузлами
   порушують просторову локальність, змушуючи процесор завантажувати нові, некешовані блоки пам'яті з RAM.
4. **Правильна відповідь: A.** Обґрунтування: Це точне визначення витоку пам'яті: виділена пам'ять, яка стала недосяжною
   і не може бути звільнена.
5. **Правильна відповідь: A.** Обґрунтування: Це основний принцип алгоритму. Якщо цикл існує, швидкий вказівник увійде в
   нього і зрештою наздожене і зустріне повільного вказівника всередині циклу.

---

## Додаток: Вихідний код тестів продуктивності на C++

```cpp
#include <iostream>
#include <vector>
#include <list>
#include <chrono>
#include <random>
#include <numeric>
#include <algorithm>
#include <iomanip>

// Використовуємо volatile sink, щоб компілятор не оптимізував цикли
volatile long long sink = 0;

// Високоточний таймер з використанням std::chrono
class Timer {
public:
    Timer() : start_time(std::chrono::high_resolution_clock::now()) {}
    double elapsed_ms() {
        auto end_time = std::chrono::high_resolution_clock::now();
        return std::chrono::duration<double, std::milli>(end_time - start_time).count();
    }
private:
    std::chrono::time_point<std::chrono::high_resolution_clock> start_time;
};

void benchmark_sequential_traversal(const std::vector<int>& vec, const std::list<int>& li) {
    std::cout << "\n--- Тест 1: Послідовний обхід ---\n";
    
    // Обхід вектора
    Timer vec_timer;
    long long vec_sum = 0;
    for (int x : vec) {
        vec_sum += x;
    }
    sink = vec_sum; // Використовуємо результат
    double vec_duration = vec_timer.elapsed_ms();
    std::cout << "Час обходу std::vector: " << std::fixed << std::setprecision(2) << vec_duration << " мс\n";

    // Обхід списку
    Timer list_timer;
    long long list_sum = 0;
    for (int x : li) {
        list_sum += x;
    }
    sink = list_sum; // Використовуємо результат
    double list_duration = list_timer.elapsed_ms();
    std::cout << "Час обходу std::list:   " << std::fixed << std::setprecision(2) << list_duration << " мс\n";
}

void benchmark_random_access(const std::vector<int>& vec, const std::list<int>& li, const std::vector<size_t>& indices) {
    std::cout << "\n--- Тест 2: Довільний доступ ---\n";

    // Довільний доступ до вектора
    Timer vec_timer;
    long long vec_sum = 0;
    for (size_t index : indices) {
        vec_sum += vec[index];
    }
    sink = vec_sum;
    double vec_duration = vec_timer.elapsed_ms();
    std::cout << "Час довільного доступу std::vector: " << std::fixed << std::setprecision(2) << vec_duration << " мс\n";

    // Довільний доступ до списку (симулюється просуванням ітератора)
    Timer list_timer;
    long long list_sum = 0;
    for (size_t index : indices) {
        auto it = li.begin();
        std::advance(it, index);
        list_sum += *it;
    }
    sink = list_sum;
    double list_duration = list_timer.elapsed_ms();
    std::cout << "Час довільного доступу std::list:   " << std::fixed << std::setprecision(2) << list_duration << " мс\n";
}

void benchmark_insertions_at_front() {
    std::cout << "\n--- Тест 3: Вставка на початок ---\n";
    const int INSERTION_COUNT = 100000;

    // Вставка на початок вектора
    Timer vec_timer;
    std::vector<int> vec_insert;
    for (int i = 0; i < INSERTION_COUNT; ++i) {
        vec_insert.insert(vec_insert.begin(), i);
    }
    double vec_duration = vec_timer.elapsed_ms();
    std::cout << "Час вставки у std::vector: " << std::fixed << std::setprecision(2) << vec_duration << " мс\n";

    // Вставка на початок списку
    Timer list_timer;
    std::list<int> list_insert;
    for (int i = 0; i < INSERTION_COUNT; ++i) {
        list_insert.push_front(i);
    }
    double list_duration = list_timer.elapsed_ms();
    std::cout << "Час вставки у std::list:   " << std::fixed << std::setprecision(2) << list_duration << " мс\n";
}

int main() {
    // Встановлюємо українську локаль для коректного виводу
    setlocale(LC_ALL, "uk_UA.UTF-8");
    
    const size_t ELEMENT_COUNT = 10000000;
    const size_t RANDOM_ACCESS_COUNT = 100000;

    std::cout << "Налаштування тестових даних з " << ELEMENT_COUNT << " елементами...\n";
    
    // Налаштування основних структур даних
    std::vector<int> main_vector(ELEMENT_COUNT);
    std::iota(main_vector.begin(), main_vector.end(), 0);
    std::list<int> main_list;
    main_list.assign(main_vector.begin(), main_vector.end());

    // Налаштування випадкових індексів для тесту доступу
    std::vector<size_t> random_indices(RANDOM_ACCESS_COUNT);
    std::mt19937 gen(1337); // Фіксоване зерно для відтворюваних результатів
    std::uniform_int_distribution<size_t> distrib(0, ELEMENT_COUNT - 1);
    for (size_t i = 0; i < RANDOM_ACCESS_COUNT; ++i) {
        random_indices[i] = distrib(gen);
    }
    
    std::cout << "Налаштування завершено. Запуск тестів...\n";
    
    benchmark_sequential_traversal(main_vector, main_list);
    benchmark_random_access(main_vector, main_list, random_indices);
    benchmark_insertions_at_front();

    return 0;
}
